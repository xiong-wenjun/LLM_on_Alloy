import argparse
import json
import torch
import tqdm
import os

import re
import signal
from typing import Optional
import numpy as np
from typing import Union
from latex2sympy2 import latex2sympy

from datasets import Dataset
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# Constants for normalization
SUBSTITUTIONS = [
    ("an ", ""),
    ("a ", ""),
    (".$", "$"),
    ("\\$", ""),
    (r"\ ", ""),
    (" ", ""),
    ("mbox", "text"),
    (",\\text{and}", ","),
    ("\\text{and}", ","),
    ("\\text{m}", "\\text{}"),
]

REMOVED_EXPRESSIONS = [
    "square",
    "ways",
    "integers",
    "dollars",
    "mph",
    "inches",
    "hours",
    "km",
    "units",
    "\\ldots",
    "sue",
    "points",
    "feet",
    "minutes",
    "digits",
    "cents",
    "degrees",
    "cm",
    "gm",
    "pounds",
    "meters",
    "meals",
    "edges",
    "students",
    "childrentickets",
    "multiples",
    "\\text{s}",
    "\\text{.}",
    "\\text{\ns}",
    "\\text{}^2",
    "\\text{}^3",
    "\\text{\n}",
    "\\text{}",
    r"\mathrm{th}",
    r"^\circ",
    r"^{\circ}",
    r"\;",
    r",\!",
    "{,}",
    '"',
    "\\dots",
]


def normalize_final_answer(final_answer: str) -> str:
    """Normalize a final answer to a quantitative reasoning question, without destroying separators."""
    # ä»…ä¿ç•™ç­‰å·å³ä¾§
    final_answer = final_answer.split("=")[-1]

    # è½»é‡æ›¿æ¢ï¼šå»æ‰æ˜æ˜¾æ— å…³çš„ LaTeX åŒ…è£¹ï¼Œä½†ä¸è¦å»ç©ºæ ¼/é€—å·
    for before, after in [
        ("mbox", "text"),
        (",$\\text{and}", ","),
        ("\\text{and}", ","),
        ("\\text{m}", "\\text{}"),
    ]:
        final_answer = final_answer.replace(before, after)

    for expr in [
        "\\ldots", "\\dots", "\\text{}^2", "\\text{}^3",
        "\\text{.}", "\\text{ }", "\\text{}"
    ]:
        final_answer = final_answer.replace(expr, "")

    # å»æ‰ \left \right ä¸å¸¸è§æ‹¬å·åŒ…è£¹
    final_answer = re.sub(r"\\left\s*\(", "(", final_answer)
    final_answer = re.sub(r"\\right\s*\)", ")", final_answer)
    final_answer = re.sub(r"\\left\s*\[", "[", final_answer)
    final_answer = re.sub(r"\\right\s*\]", "]", final_answer)
    final_answer = final_answer.replace(r"\left", "").replace(r"\right", "")

    # ä»…ä¿ç•™ç¬¬ä¸€æ®µæ•°å­¦å†…å®¹ï¼ˆè‹¥æœ‰ $...$ï¼‰ï¼Œå¦åˆ™ä¿ç•™åŸæ–‡
    m = re.search(r"\$(.*?)\$", final_answer)
    if m:
        final_answer = m.group(1)

    # å»æ‰ \text{...} / \textbf{...} / \overline{...} çš„å£³
    final_answer = re.sub(r"\\textbf\{(.*?)\}", r"\1", final_answer)
    final_answer = re.sub(r"\\text\{(.*?)\}", r"\1", final_answer)
    final_answer = re.sub(r"\\overline\{(.*?)\}", r"\1", final_answer)
    final_answer = re.sub(r"\\boxed\{(.*)\}", r"\1", final_answer)

    # è§„èŒƒåˆ†æ•°/å¼€æ–¹ç­‰ç®€å†™
    final_answer = re.sub(r"(frac)([^{])(.)", r"frac{\2}{\3}", final_answer)
    final_answer = re.sub(r"(sqrt)([^{])", r"sqrt{\2}", final_answer)

    # â€”â€”å…³é”®æ”¹åŠ¨â€”â€”
    # ä»…ç§»é™¤â€œç¡®è®¤ä¸ºåƒåˆ†ä½â€çš„é€—å·ï¼šæ•°å­—åè·Ÿé€—å·ï¼Œé€—å·åç´§è·Ÿ3ä½æ•°å­—å¹¶ä»¥éæ•°å­—æˆ–ç»“å°¾æ”¶æŸ
    final_answer = re.sub(r"(?<=\d),(?=\d{3}(?:\D|$))", "", final_answer)

    # å»æ‰é¦–å°¾ç©ºç™½ï¼›ä¸è¦ç§»é™¤ä¸­é—´ç©ºæ ¼/é€—å·
    return final_answer.strip()

################################

def verify_answer_predict(generated_answer, final_ground_truth):
    '''predict approxiamate value of accurate answer'''
    final_answer = extract_answer(generated_answer)
    if final_answer == "NA":
        return -1.0, False, final_answer  # r acc pred
    final_answer = normalize_final_answer(final_answer)
    temp = final_answer
    final_answer = extract_numeric_value(final_answer)
    if final_answer == "Format Error":
        return -1.0, False, temp

    # è®¡ç®—è¯¯å·®ç™¾åˆ†æ¯”ï¼ˆç»å¯¹å€¼ï¼‰
    error_percentage = (abs(final_answer - final_ground_truth) / (abs(final_ground_truth)+1e-7)) * 100

    # åˆ¤æ–­æ˜¯å¦åœ¨20%èŒƒå›´å†…
    is_correct = error_percentage <= 20

    # è®¡ç®—å¥–åŠ±å€¼ï¼šåœ¨20%èŒƒå›´å†…æ—¶ï¼Œå¥–åŠ±ä»0åˆ°1çº¿æ€§å˜åŒ–
    # å½“å®Œå…¨å‡†ç¡®æ—¶å¥–åŠ±ä¸º1ï¼Œå½“æ­£å¥½åœ¨20%è¾¹ç•Œä¸Šæ—¶å¥–åŠ±ä¸º0
    if is_correct:
        reward = 1.0 - (error_percentage / 20.0)
    else:
        reward = -1.0

    return reward, is_correct, temp

def extract_answer(answer):
    if "\\boxed" not in answer:
        return "NA"

    after_boxed = answer.split("\\boxed")[-1]
    paren_stack = []
    start_found = False
    start_index = None
    end_index = None
    for i, char in enumerate(after_boxed):
        if char == "{":
            if not start_found:
                start_index = i
                start_found = True
            paren_stack.append(char)
        elif char == "}":
            if paren_stack:
                paren_stack.pop()
            if start_found and len(paren_stack) == 0:
                end_index = i
                break
    if start_index is not None and end_index is not None:
        return after_boxed[start_index+1:end_index]
    else:
        return "NA"

def extract_numeric_value(input_str):
    match = re.search(r'([+-]?(?:\d+\.?\d*|\.\d+)(?:[eE][+-]?\d+)?)', input_str)

    if not match:
        raise ValueError("è¾“å…¥å­—ç¬¦ä¸²ä¸­æœªæ‰¾åˆ°æ•°å€¼éƒ¨åˆ†")

    numeric_str = match.group(1)
    numeric_value = float(numeric_str)

    if '%' in input_str:
        return numeric_value / 100.0
    else:
        return numeric_value

#
#gt = 130
#ans = '<think> 122% </think> \\boxed{130 Mpa}'
#print(compute_score(ans, gt, data_source = "material10k_magnitude"))
    

#################################

# ==============================================================================
# ä¸»è¯„æµ‹é€»è¾‘
# ==============================================================================
def main(args):
    """
    ä½¿ç”¨ vLLM è¿›è¡Œæ‰¹é‡æ¨ç†å’Œ pass@k è¯„ä¼°ã€‚
    è¯¥è„šæœ¬ä¼šè¿­ä»£æœ€å¤š k æ¬¡ï¼ˆä½¿ç”¨ä¸åŒçš„ç§å­ï¼‰ï¼Œå¹¶ä»åç»­çš„è¿è¡Œä¸­ç§»é™¤å·²ç»æˆåŠŸè§£å†³çš„é—®é¢˜ã€‚
    """
    model_name = os.path.basename(os.path.normpath(args.model_path))
    print("="*20 + f" å¼€å§‹ä¸ºæ¨¡å‹è¿›è¡Œ Pass@{args.k} è¯„ä¼°: {model_name} " + "="*20)

    # 1. è§£æç§å­åˆ—è¡¨
    try:
        seeds = [int(s.strip()) for s in args.seeds.split(',')]
        if len(seeds) < args.k:
            raise ValueError(f"æä¾›çš„ç§å­æ•°é‡ ({len(seeds)}) å°‘äº k ({args.k})ã€‚è¯·æä¾›è‡³å°‘ k ä¸ªç§å­ã€‚")
    except Exception as e:
        print(f"è§£æç§å­æ—¶å‡ºé”™: {e}")
        return

    # 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print(f"ä» {args.model_path} åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    try:
        # æ³¨æ„ï¼šä¸ºäº†å¯å¤ç°æ€§ï¼Œè¿™é‡Œçš„ seed å‚æ•°åœ¨ vLLM ä¸­ç”¨äºåˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼Œ
        # æˆ‘ä»¬å°†åœ¨æ¯æ¬¡å¾ªç¯ä¸­é€šè¿‡ sampling_params ä¼ é€’ä¸åŒçš„ç§å­ä»¥æ”¹å˜ç”Ÿæˆç»“æœã€‚
        llm = LLM(
            model=args.model_path,
            tensor_parallel_size=args.tensor_parallel_size,
            gpu_memory_utilization=args.gpu_memory_utilization,
            dtype=torch.bfloat16,
            trust_remote_code=True,
            seed=args.seed  # åŸºç¡€åˆå§‹åŒ–ç§å­
        )
        tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
        print("æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹æˆ–åˆ†è¯å™¨å¤±è´¥: {e}")
        return

    # 3. åŠ è½½æ•°æ®é›†
    print(f"ä» {args.input_file} åŠ è½½ JSON æ•°æ®...")
    try:
        dataset = Dataset.from_json(args.input_file)
        total_count = len(dataset)
        print(f"æˆåŠŸåŠ è½½ {total_count} æ¡è®°å½•ã€‚")
    except Exception as e:
        print(f"åŠ è½½ JSON æ–‡ä»¶å¤±è´¥: {e}")
        return


    # 4. åˆå§‹åŒ–ç»“æœè¿½è¸ª
    all_results = {}
    for i in range(len(dataset)):
        item_id = dataset[i]["id"]
        question_text = dataset[i]["question"]
        ground_truth = dataset[i]["ideal_answer"]

        all_results[item_id] = {
            "id": item_id,
            "prompt": [{"role": "user", "content": question_text}],
            "ground_truth": ground_truth,
            "is_correct_in_k": False,
            "solved_at_attempt": -1,
            "generations": []
        }

    unsolved_ids = set(all_results.keys())
    correct_in_k_count = 0
    batch_size = args.batch_size

    # 5. Pass@k å¾ªç¯
    for attempt in range(1, args.k + 1):
        current_seed = int(args.seeds.split(',')[attempt - 1])
        print(f"\n--- ç¬¬ {attempt}/{args.k} æ¬¡å°è¯• | ä½¿ç”¨ç§å­: {current_seed} ---")

        if not unsolved_ids:
            print("ğŸ‰ æ‰€æœ‰é—®é¢˜å‡å·²è§£å†³ï¼æå‰ç»ˆæ­¢è¯„ä¼°ã€‚")
            break

        unsolved_dataset = [d for d in dataset if d["id"] in unsolved_ids]
        print(f"å¾…è§£å†³é—®é¢˜æ•°é‡: {len(unsolved_dataset)}")

        # é…ç½®é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            n=1,
            temperature=args.temperature,
            top_p=args.top_p,
            max_tokens=args.max_tokens,
            skip_special_tokens=True,
        )

        newly_solved_in_this_attempt = 0

        # æ‰¹å¤„ç†å¾ªç¯
        for i in tqdm.tqdm(range(0, len(unsolved_dataset), batch_size), desc=f"ç¬¬ {attempt} æ¬¡å°è¯•å¤„ç†ä¸­"):
            batch_data = unsolved_dataset[i:i + batch_size]

            batch_prompts, batch_ids, batch_ground_truths = [], [], []
            for item in batch_data:
                prompt_template = r"""You are a materials science expert. Please answer the following question, and put the final answer inside the LaTeX symbol \boxed{}. The output inside \boxed{} must adhere to the following unit conventions:
- **Yield strength, tensile strength, or similar:** Numerical value (without units) if the answer is in MPa.
- **Elongation or similar:** Percentage value (with the '%' symbol).
- **Hardness or similar:** Numerical value (without units) if the answer is in HV.

Do not include units such as 'MPa','HV', or any other text in your response. Only provide the true value as specified in the ground truth. For example:
- If the ground truth is '120 MPa', output: \boxed{120}
- If the ground truth is '17.0%', output: \boxed{17.0%}
- If the ground truth is '200 HV', output: \boxed{200}
- All answers that are percentages must include the percent sign, e.g., {18\\%}.
For the remaining data types, please use common units.

Here's the format request:
Please provide your reasoning process enclosed within <think> and </think> tags, followed by your final answer with explanation. i.e., <think> your reasoning here </think> your answer here, including \boxed{final answer}
Please ensure your output is as concise as possible while maintaining accuracy and reliability. When answering, beyond recalling and remembering inherent material properties, focus more on the application of these properties and logical scientific derivation.
Let's think step by step.
"""

                batch_prompts.append(prompt_template + "\n\n" + item["question"])
                batch_ids.append(item["id"])
                batch_ground_truths.append(item["ideal_answer"])

            # è·³è¿‡ç©ºæ‰¹æ¬¡
            if not batch_prompts:
                continue

            # ç”Ÿæˆç­”æ¡ˆ
            batch_outputs = llm.generate(batch_prompts, sampling_params)

            for j, request_output in enumerate(batch_outputs):
                item_id = batch_ids[j]
                ground_truth = batch_ground_truths[j]
                generated_answer = request_output.outputs[0].text.strip()

                try:
                    # ç»Ÿä¸€ç”¨ extract_numeric_value æ¥å¤„ç† ground_truth
                    try:
                        gt_value = extract_numeric_value(ground_truth)
                    except Exception as e:
                        print(f"Ground truth {ground_truth} è§£æå¤±è´¥: {e}")
                        gt_value = float(re.search(r'([+-]?\d+\.?\d*)', ground_truth).group(1))

                    score, is_correct, model_prediction = verify_answer_predict(generated_answer, gt_value)

                except Exception as e:
                    print(f"\nè­¦å‘Š: ID {item_id} è°ƒç”¨ verify_answer_predict æ—¶å‡ºé”™: {e}")
                    is_correct = False
                    model_prediction = f"EVALUATION_ERROR: {e}"

                # è®°å½•ç”Ÿæˆç»“æœ
                generation_data = {
                    "attempt": attempt,
                    "seed": current_seed,
                    "generated_answer": generated_answer,
                    "model_prediction": model_prediction,
                    "is_correct": bool(is_correct)
                }
                all_results[item_id]["generations"].append(generation_data)

                # æ›´æ–°å·²è§£å†³çŠ¶æ€
                if is_correct and not all_results[item_id]["is_correct_in_k"]:
                    all_results[item_id]["is_correct_in_k"] = True
                    all_results[item_id]["solved_at_attempt"] = attempt
                    unsolved_ids.remove(item_id)
                    correct_in_k_count += 1
                    newly_solved_in_this_attempt += 1

        print(f"ç¬¬ {attempt} æ¬¡å°è¯•å®Œæˆã€‚æ–°è§£å†³é—®é¢˜æ•°: {newly_solved_in_this_attempt}")
        current_accuracy = correct_in_k_count / len(dataset)
        print(f"å½“å‰ Pass@{args.k} å‡†ç¡®ç‡: {current_accuracy:.4f} ({correct_in_k_count}/{len(dataset)})")

        # 6. ä¿å­˜ä¸­é—´ç»“æœ
        # ä»æœ€ç»ˆè¾“å‡ºæ–‡ä»¶åæ´¾ç”Ÿä¸­é—´æ–‡ä»¶å
        output_dir = os.path.dirname(args.output_file)
        base_name = os.path.basename(args.output_file).replace('.json', '')
        intermediate_output_path = os.path.join(output_dir, f"{base_name}_attempt_{attempt}.json")

        print(f"ä¿å­˜ç¬¬ {attempt} æ¬¡å°è¯•çš„ä¸­é—´ç»“æœåˆ° {intermediate_output_path}...")
        try:
            # åˆ›å»ºä¸­é—´è¾“å‡ºçš„å‰¯æœ¬ï¼Œä»¥ä¾¿ä¸ä¿®æ”¹ä¸»å¾ªç¯ä¸­çš„æ•°æ®ç»“æ„
            intermediate_output = {
                "model_name": model_name,
                "pass_at_k_configuration": args.k,
                "current_attempt": attempt,
                "current_seed": current_seed,
                "current_pass_rate": current_accuracy,
                "total_samples": total_count,
                "correct_samples_so_far": correct_in_k_count,
                "results": list(all_results.values()) # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿åºåˆ—åŒ–
            }
            with open(intermediate_output_path, 'w', encoding='utf-8') as f:
                json.dump(intermediate_output, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"ä¿å­˜ä¸­é—´ç»“æœæ–‡ä»¶å¤±è´¥: {e}")


    # 7. è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡å¹¶å‡†å¤‡æœ€ç»ˆè¾“å‡ºæ–‡ä»¶
    final_accuracy = (correct_in_k_count / total_count) if total_count > 0 else 0
    print("\n" + "="*20 + " è¯„ä¼°å®Œæˆ " + "="*20)
    print(f"æ¨¡å‹: {model_name}")
    print(f"æ€»æ ·æœ¬æ•°: {total_count}")
    print(f"Pass@{args.k} æ­£ç¡®æ•°: {correct_in_k_count}")
    print(f"æœ€ç»ˆ Pass@{args.k} å‡†ç¡®ç‡: {final_accuracy:.4f}")

    final_output = {
        "model_name": model_name,
        "pass_at_k": args.k,
        "accuracy": final_accuracy,
        "total_samples": total_count,
        "correct_samples": correct_in_k_count,
        "results": list(all_results.values()) # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿åºåˆ—åŒ–
    }

    # 8. ä¿å­˜æœ€ç»ˆç»“æœåˆ° JSON æ–‡ä»¶
    print(f"æ­£åœ¨ä¿å­˜æœ€ç»ˆè¯„ä¼°ç»“æœåˆ° {args.output_file}...")
    try:
        with open(args.output_file, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, ensure_ascii=False, indent=4)
        print(f"æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼è¯„ä¼°ç»“æœå·²æˆåŠŸä¿å­˜åˆ° {args.output_file}")
    except Exception as e:
        print(f"ä¿å­˜æœ€ç»ˆç»“æœæ–‡ä»¶å¤±è´¥: {e}")


if __name__ == "__main__":
    # ä¿è¯ vLLM å¯å¤ç°æ€§
    os.environ["VLLM_ENABLE_V1_MULTIPROCESSING"] = "0"

    parser = argparse.ArgumentParser(description="ä½¿ç”¨ vLLM å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œé«˜æ•ˆçš„ pass@k è¯„ä¼°ã€‚")
    
    # è·¯å¾„å‚æ•°
    parser.add_argument(
        "--model_path", 
        type=str, 
        default="/gemini/code/grpo/Qwen2.5-7B-Instruct-GRPO_step86140", 
        # default="/mnt-nfsdata/MaterialCode/base-model/DeepSeek-R1-0528-Qwen3-8B",
        help="å¾…è¯„ä¼°çš„åŸºç¡€ LLM æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--input_file", 
        type=str, 
        default="/mnt-nfsdata/zhengye/qna_dataset_pipeline/setting1/final_output/0723_val_dataset_400.json", 
        help="è¾“å…¥ .json æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output_file", 
        type=str, 
        default="/gemini/code/outputs/baseline_results.json", 
        help="æœ€ç»ˆè¾“å‡ºçš„ .json ç»“æœæ–‡ä»¶è·¯å¾„"
    )

    # vLLM æ€§èƒ½å‚æ•°
    parser.add_argument("--tensor_parallel_size", type=int, default=8, help="vLLM çš„å¼ é‡å¹¶è¡Œå¤§å°")
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.8, help="vLLM çš„ GPU å†…å­˜åˆ©ç”¨ç‡")
    parser.add_argument("--batch_size", type=int, default=64, help="æ¨ç†çš„æ‰¹å¤„ç†å¤§å°")

    # æ¨¡å‹ç”Ÿæˆä¸è¯„ä¼°å‚æ•°
    parser.add_argument("--k", "--pass_at_k", type=int, default=1, help="pass@k è¯„ä¼°ä¸­æ¯ä¸ªé—®é¢˜çš„æœ€å¤§å°è¯•æ¬¡æ•°")
    parser.add_argument("--seeds", type=str, default="74", help="ç”¨äºæ¯æ¬¡å°è¯•çš„é€—å·åˆ†éš”çš„ç§å­åˆ—è¡¨ (ä¾‹å¦‚ '321,105,421')")
    parser.add_argument("--seed", type=int, default=42, help="vLLM ç¯å¢ƒçš„å…¨å±€åˆå§‹åŒ–ç§å­")
    parser.add_argument("--temperature", type=float, default=1.0, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top_p", type=float, default=0.95, help="Top-p é‡‡æ ·")
    parser.add_argument("--max_tokens", type=int, default=14000, help="è¦ç”Ÿæˆçš„æœ€å¤§ token æ•°")
    
    args = parser.parse_args()
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(args.output_file)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"å·²åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        
    main(args)
